<!-- SECTION: Introduction -->
<section id="introduction">
  <h2>Introduction</h2>

  <p>
    As contrary to using different models to perform different tasks, modern Large Language Models (LLM) such as ChatGPT 4 use a single unified neural network backbone called the foundation model that's often based on Transformer architecture with shared weights to perform multiple tasks, such as image caption and text/image generation. Instead of having worse performance, when trained on a large corpus of data of multimodality, such as both images and texts all at once, models generally perform better compared to training on single modality. This suggests that there could be a unified representation of different modalities that describes the same object.
  </p>

  <p>
    To explain such phenomenon, the <strong>Platonic Representation Hypothesis</strong> was introduced <span class="citation">[Huh et al., 2024]</span>. To build upon this work, in this paper, we present a refined platonic representation hypothesis, taking into account of the recent work that shows neurons which fire to similar concepts tend to form local clusters that resemble lobes of the brain <span class="citation">[Anthropic, 2024]</span>.
  </p>

  <div class="key-question">
    <strong>Central Investigation</strong>
    We run experiments to demonstrate the evolution of the alignment of the model over training epoch. We observe that the first few layers align quickly, while for the later layers we observe a phase of plateau followed by a sharp transition of alignment—a phenomenon reminiscent of Grokking.
  </div>

  <p>
    The philosophical implications are profound. Just as Plato argued that our perceived reality consists of imperfect shadows of ideal "Forms," the Platonic Representation Hypothesis suggests that different models and modalities are approximating the same underlying reality from different vantage points. The authors hypothesized that there exists an idealized statistical model $P(z)$ that governs the events we observe in the universe, and the observations we make about objects are only partial information of the original object. Similar to the Cave allegory made by Plato, word descriptions and image descriptions are merely shadows of the original object.
  </p>

  <h3>The Linear Representation Hypothesis</h3>

  <p>
    It's widely accepted in the ML community that direction of the representation space embeds semantic meaning, and that we have an approximate linear relation of $\text{King} - \text{Queen} \approx \text{Man} - \text{Woman}$. Mathematically, it's regarded that each feature is represented by a unit vector $d_i$ in the representation space, and the general vector can be written as $\sum_i a_i d_i$. This is known as the <strong>linear representation hypothesis</strong>. However, this relation only holds approximately.
  </p>

  <h3>Scales of Representation Space</h3>

  <p>
    To further understand the representation space, tools such as Sparse Auto-Encoder (SAE) were introduced, where we take the first k layers of the LLM and introduce an Auto-Encoder with the loss designed to make the features sparse. We denote $x$ to be the input to SAE, $f(x) = \text{Encoder}(x)$ to be the latent representation, and $\hat{x} = \text{Decoder}(f)$ to be the reconstructed vector. We freeze the first k layers and train the head by the following loss:
  </p>

  <div class="math-block">
    $$\mathcal{L} = \mathbb{E}_{x \sim \mathcal{D}}\left[ ||x - \hat{x}||_2^2 + \lambda ||f||_1 \right]$$
    <div class="caption">
      The first term is the reconstruction loss ensuring the autoencoder preserves information. The second term enforces sparsity, encouraging the model to use only a few features at once.
    </div>
  </div>

  <p>
    Using SAE, it's been explored that there are <strong>three scales</strong> of the representation space: Atomic, Intermediate, and Galaxy scales. The atomic scale contains crystal structures with parallelogram or trapezoid faces that approximately satisfy the linear representation hypothesis. In the intermediate scale, it's observed that neurons that fire to similar concepts, such as coding and math, are clustered together—similar to lobes of the brain seen in fMRI images.
  </p>

  <div class="insight">
    Nearby vectors in representation space are observed to have similar semantic meanings. This suggests a sense of <em>smoothness</em> of the embedding of datapoints for large enough models trained over many epochs. Given a point $x$ in the representation space and sufficiently small radius $\epsilon > 0$, all the points in the ball $B_\epsilon(x) \equiv \{y \mid |x - y| < \epsilon\}$ have similar semantic meanings.
  </div>
</section>
