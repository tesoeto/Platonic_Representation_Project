<!-- Section: References -->
<section id="references">
  <h2>References</h2>

  <ol>
    <li>
      <strong>Huh, M., Cheung, B., Wang, T., & Isola, P. (2024).</strong>
      <em>The Platonic Representation Hypothesis.</em>
      International Conference on Machine Learning (ICML).
      <br><span class="citation-note">Introduces the PRH and presents evidence for convergent representations across modalities and architectures.</span>
    </li>
    <li>
      <strong>Biderman, S., Schoelkopf, H., Anthony, Q., et al. (2023).</strong>
      <em>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling.</em>
      International Conference on Machine Learning (ICML).
      <br><span class="citation-note">Provides the model suite and checkpoints used throughout our experiments.</span>
    </li>
    <li>
      <strong>Kornblith, S., Norouzi, M., Lee, H., & Hinton, G. (2019).</strong>
      <em>Similarity of Neural Network Representations Revisited.</em>
      International Conference on Machine Learning (ICML).
      <br><span class="citation-note">Introduces CKA as a metric for comparing neural network representations.</span>
    </li>
    <li>
      <strong>Gretton, A., Bousquet, O., Smola, A., & Schölkopf, B. (2005).</strong>
      <em>Measuring Statistical Dependence with Hilbert-Schmidt Norms.</em>
      International Conference on Algorithmic Learning Theory.
      <br><span class="citation-note">Foundational work on HSIC, the basis for CKA.</span>
    </li>
    <li>
      <strong>Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022).</strong>
      <em>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.</em>
      International Conference on Learning Representations (ICLR) Workshop.
      <br><span class="citation-note">Documents the grokking phenomenon that inspired our investigation of non-monotonic learning dynamics.</span>
    </li>
    <li>
      <strong>Anthropic (2023).</strong>
      <em>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</em>
      <br><span class="citation-note">Reveals multi-scale structure in neural representations using Sparse Autoencoders, motivating our refined hypothesis.</span>
    </li>
    <li>
      <strong>Mikolov, T., Yih, W., & Zweig, G. (2013).</strong>
      <em>Linguistic Regularities in Continuous Space Word Representations.</em>
      NAACL-HLT.
      <br><span class="citation-note">Foundational work on the linear structure of word embeddings (King - Queen ≈ Man - Woman).</span>
    </li>
    <li>
      <strong>Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2017).</strong>
      <em>Pointer Sentinel Mixture Models.</em>
      International Conference on Learning Representations (ICLR).
      <br><span class="citation-note">Introduces the WikiText dataset used in our experiments.</span>
    </li>
  </ol>

  <style>
    .citation-note {
      font-size: 0.85rem;
      color: var(--color-text-muted);
      display: block;
      margin-top: 0.25rem;
      margin-left: 1rem;
    }
  </style>
</section>
