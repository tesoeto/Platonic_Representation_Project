<!-- SECTION: Experiment Results -->
<section id="results">
  <h2>Experiment Results</h2>

  <h3>Training Dynamics: The Surprising Non-Monotonicity of CKA</h3>

  <p>
    Our first experiment tracks how representation alignment evolves throughout training. We compare each checkpoint's representation against the final trained model, asking: "How close is this intermediate representation to what the model eventually learns?"
  </p>

  <figure class="figure-wide">
    <img src="assets/images/Unknown-14.png" alt="Representation convergence analysis showing CKA and CKNNA metrics over training steps">
    <figcaption>
      <strong>Figure 1: Training Dynamics at the Last Layer.</strong>
      Both CKA (blue) and CKNNA (orange, k=16) are plotted against training step count. The striking finding: CKNNA starts high (~0.995) and remains remarkably stable throughout training, while CKA shows dramatic non-monotonic behavior—rising initially, dropping sharply around step 20,000-40,000, then recovering. This dissociation reveals that <em>global</em> structure undergoes significant reorganization while <em>local</em> neighborhood structure remains preserved.
    </figcaption>
  </figure>

  <h4>What Does This Divergence Mean?</h4>

  <p>
    The dramatic difference between CKA and CKNNA reveals something profound about how neural networks learn. Around step 20,000-40,000, the model undergoes what we call a <strong>global reorganization phase</strong>:
  </p>

  <ul>
    <li><strong>Local structure is preserved:</strong> CKNNA remains above 0.995 throughout training. The model consistently identifies which data points are similar to each other—the "who is near whom" question has a stable answer even as the overall geometry shifts.</li>
    <li><strong>Global structure is reorganized:</strong> CKA drops to ~0.982, indicating the overall shape of the representation manifold is being restructured. Distant points are changing their relationships even as nearby points maintain theirs.</li>
    <li><strong>Eventual convergence:</strong> Both metrics converge toward 1.0 by the end of training, confirming that the final representation achieves both local and global stability.</li>
  </ul>

  <div class="insight">
    Think of it like rearranging furniture in a house: the rooms (local neighborhoods) keep their internal arrangement, but the house itself (global structure) is being remodeled. Points that were close stay close, but the overall geometry—the "floor plan"—transforms.
  </div>

  <h3>Deep Dive: Kernel Matrix Analysis</h3>

  <p>
    To understand what drives the CKA drop, we examine the kernel matrices at two key checkpoints: early training (step 256) and mid-training (step 4000).
  </p>

  <figure class="figure-wide">
    <img src="assets/images/Unknown-11.png" alt="Detailed kernel analysis at checkpoint 256">
    <figcaption>
      <strong>Figure 2: Kernel Analysis at Checkpoint 256 (Early Training).</strong>
      <strong>Top row:</strong> Kernel matrices K (checkpoint 256), L (final checkpoint), and mutual kNN mask (k=16). The checkered pattern in K and L reveals cluster structure in the data.
      <strong>Bottom row:</strong> Scatter plots showing local pairs correlation (r=0.995) versus global pairs correlation (r=0.965), plus the absolute difference |K-L|. The 3% gap between local and global correlation is the signature of global reorganization—nearby points agree strongly, but distant points show more divergence.
    </figcaption>
  </figure>

  <p>
    The kernel matrices reveal a clear structure: both K and L show similar "checkered" patterns indicating cluster organization in the data. The bright spots along the diagonal and at certain off-diagonal positions correspond to groups of semantically similar text passages. What's remarkable is how similar these structures are even at step 256—the model has already learned which points belong together.
  </p>

  <figure class="figure-wide">
    <img src="assets/images/Unknown-12.png" alt="Detailed kernel analysis at checkpoint 4000">
    <figcaption>
      <strong>Figure 3: Kernel Analysis at Checkpoint 4000 (Mid-Training).</strong>
      Same analysis as Figure 2 but at step 4000. The global pairs correlation has improved to 0.980 (up from 0.965), while local pairs remain at 0.994. The |K-L| difference map shows similar structure to checkpoint 256, but with generally smaller magnitudes. The model is converging toward the final representation, with global structure catching up to local structure.
    </figcaption>
  </figure>

  <h4>The Local-Global Gap</h4>

  <p>
    The scatter plots in Figures 2 and 3 reveal the key insight: <strong>local pairs (mutual k-NN) show tighter correlation than global pairs (all pairs)</strong>. At checkpoint 256:
  </p>

  <ul>
    <li>Local pairs correlation: <strong>r = 0.995</strong></li>
    <li>Global pairs correlation: <strong>r = 0.965</strong></li>
  </ul>

  <p>
    This 3% gap explains exactly why CKNNA stays high while CKA dips. CKNNA only "sees" the local pairs where agreement is near-perfect. CKA sees all pairs, including the global ones that are still being reorganized.
  </p>

  <h3>Layer-Specific Analysis: Middle vs. Final Layers</h3>

  <p>
    The Platonic Representation Hypothesis suggests that later layers should show different convergence dynamics, as they contain more abstract, task-relevant representations. We test this by analyzing convergence at layer 12 (middle of the network).
  </p>

  <figure class="figure-wide">
    <img src="assets/images/Unknown-13.png" alt="Representation convergence at layer 12">
    <figcaption>
      <strong>Figure 4: Convergence Analysis at Layer 12 (Middle Layer).</strong>
      The same CKA/CKNNA analysis at a middle layer shows qualitatively different dynamics. Most notably: <strong>no reorganization dip</strong>. Both metrics start high and converge smoothly toward 1.0. The representation restructuring we observed in the final layer doesn't appear here.
    </figcaption>
  </figure>

  <h4>Why Do Layers Behave Differently?</h4>

  <p>
    The contrast between layer 12 and the final layer tells us something important about how information flows through the network:
  </p>

  <ul>
    <li><strong>Middle layers stabilize earlier:</strong> Layer 12 shows smooth convergence without the dramatic reorganization phase. The representations here reach their final form more directly.</li>
    <li><strong>Final layers are more dynamic:</strong> The last layer undergoes more substantial restructuring during training. This makes sense—later layers are responsible for task-specific processing and must adapt more dramatically to capture the full complexity of the training objective.</li>
    <li><strong>Early convergence of local structure:</strong> In both layers, CKNNA is consistently above CKA, confirming that local structure converges before global structure regardless of layer depth.</li>
  </ul>

  <div class="insight">
    This layer-wise analysis suggests a <em>hierarchical</em> convergence process: earlier layers establish stable, reusable features early in training, while later layers continue to reorganize how these features are combined for the final task.
  </div>

  <h3>Model Scale: Larger Models, More Aligned Representations</h3>

  <p>
    Our second experiment tests the core prediction of the Platonic Representation Hypothesis: that larger models should converge to more similar representations.
  </p>

  <figure class="figure-wide">
    <img src="assets/images/Unknown-10.png" alt="Representation alignment for models of different sizes">
    <figcaption>
      <strong>Figure 5: Representation Alignment Across Model Scales.</strong>
      We compare final-checkpoint representations across Pythia models of different sizes, plotting representation alignment (y-axis) against log model size (x-axis). Using 400 samples with k=100 for CKNNA. Both CKA and CKNNA show a clear trend: <strong>larger models produce more aligned representations</strong>. CKNNA (k=16 and k=100) consistently exceeds CKA, confirming that local structure is more aligned than global structure even across model scales.
    </figcaption>
  </figure>

  <h4>Key Observations on Scale</h4>

  <ul>
    <li><strong>Monotonic improvement with scale:</strong> Both CKA and CKNNA increase as model size grows. The 2.8B parameter model shows near-perfect alignment (>0.999) while the 70M model shows lower alignment (~0.983 for CKA).</li>
    <li><strong>CKNNA consistently higher:</strong> Across all model sizes, CKNNA exceeds CKA by 1-2%. This confirms that local neighborhoods are better aligned than global structure, even when comparing fully-trained models.</li>
    <li><strong>Diminishing returns at scale:</strong> The gap between consecutive model sizes shrinks as we move to larger models. The jump from 70M to 160M is substantial; the jump from 1.4B to 2.8B is smaller.</li>
  </ul>

  <div class="insight">
    The scale results support the Platonic Representation Hypothesis: as models become more capable, they converge toward similar representations. The local-before-global pattern we observed in training dynamics also appears in the scale comparison—local structure is more universal than global structure.
  </div>

  <h3>Connecting to Grokking</h3>

  <p>
    The non-monotonic behavior of CKA during training is reminiscent of <strong>grokking</strong>—the phenomenon where models suddenly generalize long after achieving perfect training accuracy. The pattern we observe suggests:
  </p>

  <ol>
    <li><strong>Initial learning phase (steps 0-15k):</strong> Rapid improvement in both local and global alignment. The model is learning basic structure.</li>
    <li><strong>Reorganization phase (steps 20k-50k):</strong> Global structure is reorganized while local structure is preserved. This may correspond to a transition from memorization to generalization—the model keeps its learned associations but reorganizes how they fit together.</li>
    <li><strong>Convergence phase (steps 50k+):</strong> Both metrics converge toward 1.0 as the representation stabilizes into its final form.</li>
  </ol>

  <div class="insight">
    The preservation of local structure during global reorganization suggests that the model maintains useful learned features while restructuring how they relate to each other. This is exactly what we'd expect during a transition from memorization to generalization: the "facts" (local relationships) stay the same, but the "theory" (global organization) changes.
  </div>
</section>
