<!-- SECTION: Conclusion -->
<section id="conclusion">
  <h2>Conclusion</h2>

  <h3>Key Findings</h3>

  <p>
    Our experiments reveal several important insights about representational dynamics during neural network training:
  </p>

  <ol>
    <li>
      <strong>Local before global:</strong> Models establish local similarity structure early in training, while global geometry continues to evolve. CKNNA consistently exceeds CKA throughout training, confirming that local neighborhoods align before—and more completely than—global structure.
    </li>
    <li>
      <strong>Non-monotonic convergence:</strong> Representation alignment is not monotonic. At the final layer, we observe a distinct reorganization phase (around steps 20k-50k) where global structure changes significantly while local structure is preserved. This suggests the model undergoes a "restructuring" phase during training.
    </li>
    <li>
      <strong>Layer-specific dynamics:</strong> Middle layers (layer 12) converge smoothly without the dramatic reorganization seen in final layers. This suggests a hierarchical learning process where earlier layers stabilize first.
    </li>
    <li>
      <strong>Scale enhances alignment:</strong> Larger models produce more aligned representations, supporting the core claim of the Platonic Representation Hypothesis. The local-before-global pattern persists across scales.
    </li>
  </ol>

  <h3>Novel Contributions</h3>

  <ul>
    <li>
      <strong>Training dynamics perspective:</strong> We apply representation analysis to study <em>learning dynamics</em>, not just compare trained models. This reveals structure in how representations evolve that static comparisons miss.
    </li>
    <li>
      <strong>Connection to grokking:</strong> The non-monotonic alignment pattern—particularly the preservation of local structure during global reorganization—suggests a connection between representation dynamics and generalization transitions. This opens new avenues for understanding grokking through the lens of representational geometry.
    </li>
    <li>
      <strong>Refined local alignment hypothesis:</strong> Our theoretical framework suggests that convergent representations should be understood primarily as <em>local</em> phenomena, with global alignment emerging as a secondary effect of scale and capacity.
    </li>
  </ul>

  <h3>Future Directions</h3>

  <ul>
    <li>
      <strong>Systematic scale study:</strong> Compare representation convergence across all Pythia sizes with finer checkpoint granularity to quantify how model capacity affects both the rate and final degree of alignment.
    </li>
    <li>
      <strong>Explicit grokking experiments:</strong> Run experiments on algorithmic tasks (modular arithmetic, group operations) where grokking is known to occur, tracking CKA/CKNNA to see if representation metrics can predict the grokking transition.
    </li>
    <li>
      <strong>Cross-modal analysis:</strong> Test whether vision and language models show similar local-before-global convergence patterns, and whether their representations become more aligned at larger scales.
    </li>
    <li>
      <strong>Theoretical analysis:</strong> Develop a formal understanding of why local structure is preserved during global reorganization—what loss landscape properties or optimization dynamics produce this pattern?
    </li>
  </ul>

  <div class="insight">
    We also provide a short speculation of the reason behind different performances of the kernels. At the end, we provide a speculation of the hypothesis that requires further works to be done. We hope this work inspires further investigation into the dynamics of representation learning and the nature of convergent representations in neural networks.
  </div>
</section>
