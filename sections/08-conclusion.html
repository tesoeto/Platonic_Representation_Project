<!-- Section: Conclusion -->
<section id="conclusion">
  <h2>Conclusion</h2>

  <p>
    We set out to understand how neural network representations evolve during training—not just where they end up, but how they get there. Our journey revealed surprising structure in the learning dynamics, with implications for the Platonic Representation Hypothesis and beyond.
  </p>

  <h3>What We Found</h3>

  <p>
    Three key findings emerge from our experiments:
  </p>

  <h4>1. Local Before Global</h4>
  <p>
    Models establish local similarity structure early in training and preserve it throughout. Global geometry continues to evolve long after local neighborhoods have stabilized. This isn't just a curiosity—it suggests that models first learn <em>what things are similar</em> before organizing <em>how those similarities relate</em> to each other globally.
  </p>

  <h4>2. Non-Monotonic Convergence</h4>
  <p>
    Representation alignment doesn't improve smoothly. There's a distinct "reorganization phase" (visible in our CKA curves) where global structure changes significantly while local structure is preserved. This may correspond to a transition from memorization to generalization—echoing the phenomenon of grokking observed in other contexts.
  </p>

  <h4>3. Metrics Reveal Different Stories</h4>
  <p>
    CKA and CKNNA capture fundamentally different aspects of representational similarity. CKA sees the global picture; CKNNA focuses on local neighborhoods. Using both together reveals dynamics that either alone would miss: the stable local alignment underlying a dramatically changing global structure.
  </p>

  <h3>Novel Contributions</h3>

  <p>
    Our work makes several contributions to the study of neural network representations:
  </p>

  <ul>
    <li><strong>Dynamics perspective:</strong> We apply representation analysis to study learning dynamics, not just compare fixed models. This reveals structure in <em>how</em> representations evolve that static comparisons miss entirely.</li>
    <li><strong>Connection to grokking:</strong> The non-monotonic alignment pattern suggests a connection between representation reorganization and generalization transitions. This opens new avenues for understanding and potentially predicting grokking.</li>
    <li><strong>Refined hypothesis:</strong> We propose that the Platonic Representation Hypothesis operates primarily at the local level. Different models may organize globally differently while agreeing locally—and local agreement may be what matters for transfer and generalization.</li>
  </ul>

  <h3>Broader Implications</h3>

  <h4>For Understanding Neural Networks</h4>
  <p>
    The stable local / dynamic global pattern suggests a hierarchical learning process. Perhaps the brain and neural networks share this pattern: first establish what's similar to what (local), then figure out the big picture (global). This might be a universal feature of learning systems operating under resource constraints.
  </p>

  <h4>For Multimodal AI</h4>
  <p>
    If local alignment is what matters, then stitching together representations from different modalities might be easier than expected. We don't need perfect global correspondence—just agreement on local neighborhoods. This has practical implications for building systems that combine vision, language, and other modalities.
  </p>

  <h4>For Interpretability</h4>
  <p>
    The lobe structure of representations—with local coherence and global organization—provides a framework for understanding what neural networks learn. Interpretability efforts might focus on identifying these lobes and understanding what each represents, rather than trying to decode individual neurons.
  </p>

  <h3>Future Directions</h3>

  <p>
    Our work opens several directions for future research:
  </p>

  <ul>
    <li><strong>Systematic scale study:</strong> How does model size affect the rate of convergence and the magnitude of the reorganization phase? Our data hints at trends, but comprehensive experiments across the full Pythia scale would be valuable.</li>
    <li><strong>Explicit grokking experiments:</strong> Apply our metrics to algorithmic tasks where grokking has been observed. Can we predict when grokking will occur by watching representational dynamics?</li>
    <li><strong>Cross-modal analysis:</strong> Do vision and language models show the same local-before-global pattern? If so, can we identify corresponding lobes across modalities?</li>
    <li><strong>Theoretical foundations:</strong> Why does local structure stabilize before global? Is there a theoretical principle—perhaps related to optimization geometry or implicit regularization—that predicts this pattern?</li>
  </ul>

  <h3>Final Thoughts</h3>

  <p>
    The Platonic Representation Hypothesis offers a beautiful vision: that all capable models are approximating the same underlying reality, converging toward common representations. Our work suggests this convergence has rich structure—it happens locally first, globally later, and proceeds through distinct phases.
  </p>

  <p>
    Understanding this structure brings us closer to understanding learning itself. And that understanding may one day help us build systems that learn more like we do: establishing local relationships, then organizing them into coherent global pictures, and sometimes—after long plateaus—suddenly "grokking" deeper structure we didn't know was there.
  </p>
</section>
