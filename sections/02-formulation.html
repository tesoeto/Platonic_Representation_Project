<!-- SECTION: Platonic Representation Formulation -->
<section id="formulation">
  <h2>Platonic Representation Formulation</h2>

  <p>
    We provide an overview of previous established concepts in literatures. A vector representation of an input $x \in \mathcal{X}$ is defined to be a mapping from the data space $\mathcal{X}$ to the target space $\mathbb{R}^n$, provided by the formula:
  </p>

  <div class="math-block">
    $$f: \mathcal{X} \rightarrow \mathbb{R}^n$$
    <div class="caption">
      The representation function maps each input from the data space to a point in n-dimensional Euclidean space. This is the fundamental object we study when comparing models.
    </div>
  </div>

  <p>
    To characterize the representation space, kernel $\mathcal{K}$ is introduced, which measures the similarity between the representations of the data points as:
  </p>

  <div class="math-block">
    $$\mathcal{K}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}, \quad \mathcal{K}(x_i, x_j) = \langle f(x_i), f(x_j) \rangle$$
    <div class="caption">
      The kernel $K(x_i, x_j)$ calculates the similarities between the different representations of the datapoints using inner product. This gives us a way to compare points without directly comparing high-dimensional vectors.
    </div>
  </div>

  <p>
    Then, given two models with different representations $f$ and $g$, we can calculate the similarity between the representations by designing a similarity metric between the two kernels $m: \mathcal{K} \times \mathcal{K} \rightarrow \mathbb{R}$. Common similarity metrics include the CKA and nearest neighbor metric, while in the original platonic representation work, the authors used CKNNA.
  </p>

  <h3>Empirical Observations on Scale</h3>

  <p>
    Previous work showed that given sufficient amount of data, as we scale up two different models A and B, the representations are more aligned for the larger models compared to the smaller models. The authors provided the following hypotheses to explain for such observation:
  </p>

  <div class="hypothesis-box">
    <div class="label">Hypothesis 1: Convergence via Task Generality</div>
    <p>
      As we increase the number of data points, the number of constraints put on the model increases if it performs well across the dataset. This restricts the possible hypothesis space to a small region. Think of it like solving a system of equationsâ€”more equations (data points) constrain the solution more tightly.
    </p>
  </div>

  <div class="hypothesis-box">
    <div class="label">Hypothesis 2: Convergence via Model Capacity</div>
    <p>
      As we increase the model capacity, the representation of the model increases so it has the capacity to find a more optimal solution. Larger models can explore a richer function space and are more likely to discover the "true" underlying structure.
    </p>
  </div>

  <div class="hypothesis-box">
    <div class="label">Hypothesis 3: The Simplicity Bias</div>
    <p>
      Larger models tend to find simpler solutions to the problems. This connects to the phenomenon of Grokking, where models first memorize the dataset then, as training continues, tend toward simpler, more generalizable solutions.
    </p>
  </div>

  <div class="insight">
    It's plausible that the third hypothesis is related to Grokking, where the model first remembers the datasets then as the training epoch increases, the model tends to a simpler solution. This suggests some form of implicit regularization driving models toward more compressed representations.
  </div>
</section>
