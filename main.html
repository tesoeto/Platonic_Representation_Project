<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Evolution of Representation Space | Platonic Representations</title>

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&family=JetBrains+Mono:wght@400;500&family=Source+Sans+3:wght@400;500;600;700&display=swap" rel="stylesheet">

  <!-- Main Stylesheet -->
  <link rel="stylesheet" href="styles.css">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="progress-bar" id="progress"></div>

  <header>
    <div class="header-content">
      <span class="category-tag">Deep Learning · Representation Learning · Training Dynamics</span>
      <h1>Evolution of Representation Space: Front and Back</h1>
      <p class="subtitle">
        Investigating how neural network representations evolve during training, and what this tells us about the Platonic Representation Hypothesis
      </p>
      <div class="meta">
        <span class="authors">Tony Wu & Baiyu Zhu</span> · Graduate Course Final Project · December 2024
      </div>
    </div>
  </header>

  <nav>
    <div class="nav-content">
      <a href="#introduction" class="active">Introduction</a>
      <a href="#formulation">Formulation</a>
      <a href="#methodology">Methodology</a>
      <a href="#research-direction">Research Direction</a>
      <a href="#experiments">Experiments</a>
      <a href="#results">Results</a>
      <a href="#refined-hypothesis">Refined Hypothesis</a>
      <a href="#conclusion">Conclusion</a>
      <a href="#references">References</a>
    </div>
  </nav>

  <main>
    <div class="toc">
      <h3>Contents</h3>
      <ol>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#formulation">Platonic Representation Formulation</a></li>
        <li><a href="#methodology">Methodology: Measuring Similarity</a></li>
        <li><a href="#research-direction">Our Research Direction</a></li>
        <li><a href="#experiments">Experiment Setup</a></li>
        <li><a href="#results">Experiment Results</a></li>
        <li><a href="#refined-hypothesis">Refining the Hypothesis</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
        <li><a href="#references">References</a></li>
      </ol>
    </div>

    <!-- ==================== INTRODUCTION ==================== -->
    <section id="introduction">
      <h2>Introduction</h2>

      <div class="key-question">
        <strong>The Central Puzzle</strong>
        Why do neural networks trained on completely different data—text, images, audio—end up learning strikingly similar internal representations?
      </div>

      <h3>From Many Models to One Foundation</h3>

      <p>
        The old paradigm of machine learning was straightforward: different tasks required different models. Image classification used CNNs, language modeling used RNNs, speech recognition had its own architectures. Each domain developed in isolation, with specialized inductive biases baked into the model design.
      </p>

      <p>
        Modern Large Language Models like GPT-4 and Claude shatter this paradigm entirely. A single unified neural network backbone—the <strong>foundation model</strong>—built on the Transformer architecture with shared weights can perform image captioning, text generation, code synthesis, and much more.
      </p>

      <p>
        Here's what makes this deeply surprising: instead of suffering from the jack-of-all-trades problem, these multimodal models actually <em>perform better</em> than single-modality specialists when trained on large, diverse corpora. Training on both images and text doesn't dilute the model's language understanding—it enhances it.
      </p>

      <div class="insight">
        This observation points to something profound: there may exist a <strong>unified representation</strong> of different modalities that captures the same underlying objects and concepts. Images of cats and descriptions of cats might converge to similar internal codes.
      </div>

      <h3>The Platonic Representation Hypothesis</h3>

      <p>
        To explain this phenomenon, Huh et al. introduced the <strong>Platonic Representation Hypothesis (PRH)</strong>. The philosophical reference is deliberate and illuminating.
      </p>

      <div class="analogy">
        <strong>Plato's Cave Allegory:</strong> Prisoners in a cave see only shadows on a wall, cast by objects passing before a fire. The shadows are imperfect projections of true Forms that exist in an idealized realm. Similarly, word descriptions and image pixels are merely <em>shadows</em> of the true objects they represent—partial, noisy observations of an underlying reality.
      </div>

      <p>
        The PRH formalizes this intuition: there exists an idealized statistical model $P(z)$ that governs events in our universe. Every observation we make—whether through language, vision, or any other modality—provides only partial information about these latent objects $z$. A sufficiently capable model, trained to predict these observations accurately, must discover structure in the underlying $z$ itself.
      </p>

      <h3>Why Does Convergence Happen?</h3>

      <p>
        The original PRH paper proposed three complementary mechanisms driving representation convergence:
      </p>

      <div class="hypothesis-box">
        <div class="label">Hypothesis 1: Convergence via Task Generality</div>
        <p>
          As we increase the number of data points and tasks, the constraints on the model multiply. To perform well across a massive, diverse dataset, the model's hypothesis space shrinks dramatically.
        </p>
      </div>

      <div class="hypothesis-box">
        <div class="label">Hypothesis 2: Convergence via Model Capacity</div>
        <p>
          Larger models have the capacity to discover more optimal solutions. A small model might settle for a hacky shortcut. A large model can find the elegant, generalizable solution that aligns with reality itself.
        </p>
      </div>

      <div class="hypothesis-box">
        <div class="label">Hypothesis 3: The Simplicity Bias</div>
        <p>
          Larger models tend to find <em>simpler</em> solutions, even though they have capacity for more complex ones. Perhaps the optimization landscape of larger models favors smooth, compressible representations.
        </p>
      </div>

      <h3>Connecting to the Structure of Representations</h3>

      <p>
        The ML community has long accepted that <strong>directions in representation space encode semantic meaning</strong>. The famous example: $\text{King} - \text{Queen} \approx \text{Man} - \text{Woman}$ suggests that gender is encoded as a consistent direction.
      </p>

      <p>
        Recent work from Anthropic using <strong>Sparse Autoencoders (SAEs)</strong> reveals a richer picture. The representation space has structure at multiple scales:
      </p>

      <ul>
        <li><strong>Atomic scale:</strong> Local crystal-like structures with parallelogram faces.</li>
        <li><strong>Intermediate scale:</strong> Neurons that fire to similar concepts cluster together in lobes, reminiscent of brain structure.</li>
        <li><strong>Galaxy scale:</strong> Global organization of these clusters into larger patterns.</li>
      </ul>

      <h3>Our Contribution: The Dynamics of Convergence</h3>

      <p>
        Prior work on the PRH has focused on comparing <em>final trained models</em>. But this leaves out a crucial dimension: <strong>how</strong> do representations evolve during training to reach convergence?
      </p>

      <p>
        In this work, we investigate the <em>dynamics</em> of representation convergence. We ask: At what point during training does alignment happen? Are there distinct phases—perhaps resembling <em>grokking</em>? Do different layers converge at different rates?
      </p>
    </section>

    <!-- ==================== FORMULATION ==================== -->
    <section id="formulation">
      <h2>Platonic Representation Formulation</h2>

      <p>
        Before we can measure whether representations are converging, we need precise mathematical language for what a "representation" even is, and how to compare two of them.
      </p>

      <h3>What Is a Representation?</h3>

      <p>
        A neural network transforms raw inputs into internal codes. Formally, a <strong>vector representation</strong> of an input $x \in \mathcal{X}$ is a mapping from the data space to a target vector space:
      </p>

      <div class="math-block">
        $$f: \mathcal{X} \rightarrow \mathbb{R}^n$$
        <div class="caption">
          The function $f$ takes any input and produces an $n$-dimensional vector—the model's internal "understanding" of that input.
        </div>
      </div>

      <h3>The Problem: How to Compare Different Spaces?</h3>

      <p>
        Here's the fundamental challenge: if model A produces 768-dimensional representations and model B produces 1024-dimensional representations, we can't directly compare their vectors. Even with the same dimension, the axes might mean completely different things.
      </p>

      <div class="insight">
        The key insight: we don't care about the <em>absolute</em> representation of any single point. We care about <strong>relationships between points</strong>. If two models agree that "cat" is similar to "dog" but distant from "democracy," they've captured something in common.
      </div>

      <h3>Kernels: Measuring Similarity Structure</h3>

      <p>
        A <strong>kernel function</strong> measures similarity between representations:
      </p>

      <div class="math-block">
        $$\mathcal{K}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}, \quad \mathcal{K}(x_i, x_j) = \langle f(x_i), f(x_j) \rangle$$
        <div class="caption">
          The kernel $\mathcal{K}(x_i, x_j)$ computes the inner product between representations. High values mean the model considers these inputs similar.
        </div>
      </div>

      <p>
        For a dataset of $n$ points, we organize all pairwise similarities into a <strong>kernel matrix</strong> $K \in \mathbb{R}^{n \times n}$ where $K_{ij} = \mathcal{K}(x_i, x_j)$.
      </p>

      <div class="analogy">
        <strong>Intuition:</strong> The kernel matrix is like a social network where edge weights indicate how much two people have in common. Two models might organize their coordinates differently, but if their "social graphs" are similar, they've learned similar structure.
      </div>

      <h3>The Right Invariances</h3>

      <p>
        A good similarity metric should be invariant under:
      </p>

      <ul>
        <li><strong>Orthogonal transformations:</strong> Rotations and reflections shouldn't change similarity.</li>
        <li><strong>Isotropic scaling:</strong> Multiplying all representations by a constant shouldn't matter.</li>
      </ul>

      <p>
        This leads us to <strong>Centered Kernel Alignment (CKA)</strong> as our primary metric.
      </p>
    </section>

    <!-- ==================== METHODOLOGY ==================== -->
    <section id="methodology">
      <h2>Methodology: Measuring Representational Similarity</h2>

      <p>
        How do we quantify whether two representations capture "the same information"? This section develops the mathematical machinery.
      </p>

      <h3>HSIC: The Hilbert-Schmidt Independence Criterion</h3>

      <p>
        Classical correlation only captures <em>linear</em> relationships. HSIC captures <em>all</em> forms of dependence by embedding distributions into a Hilbert space.
      </p>

      <div class="hypothesis-box">
        <div class="label">Theorem (Gretton et al., 2005)</div>
        <p>
          For characteristic kernels, $\text{HSIC}(X, Y) = 0$ if and only if $X$ and $Y$ are statistically independent.
        </p>
      </div>

      <p>
        HSIC measures the covariance between kernel evaluations:
      </p>

      <div class="math-block">
        $$\text{HSIC}(X, Y) = \text{Cov}_{(x,y), (x',y')}\big[k(x, x'), l(y, y')\big]$$
        <div class="caption">
          <strong>Interpretation:</strong> If $X$ and $Y$ are dependent, similar $x$-values should co-occur with similar $y$-values.
        </div>
      </div>

      <h4>The Empirical Estimator</h4>

      <div class="math-block">
        $$\widehat{\text{HSIC}}(K, L) = \frac{1}{(n-1)^2} \text{tr}(KHLH)$$
        <div class="caption">
          $H = I_n - \frac{1}{n}\mathbf{1}\mathbf{1}^\top$ is the centering matrix.
        </div>
      </div>

      <h3>From HSIC to CKA: Normalization</h3>

      <div class="math-block">
        $$\text{CKA}(K, L) = \frac{\text{HSIC}(K, L)}{\sqrt{\text{HSIC}(K, K) \cdot \text{HSIC}(L, L)}}$$
        <div class="caption">
          CKA is bounded between 0 and 1. CKA = 1 means perfect alignment.
        </div>
      </div>

      <p>
        CKA can also be interpreted as the cosine of the angle between centered kernel matrices when viewed as vectors.
      </p>

      <h3>The Problem with CKA: Global Domination</h3>

      <p>
        CKA is dominated by <strong>global structure</strong>—particularly the largest principal components. Two representations might have different local structure but similar global clusters, leading to high CKA.
      </p>

      <h3>CKNNA: Focusing on Local Structure</h3>

      <p>
        <strong>Centered Kernel Nearest Neighbor Alignment (CKNNA)</strong> restricts comparison to local neighborhoods.
      </p>

      <div class="definition">
        <div class="term">Definition: Mutual k-NN</div>
        <p>
          Points $i$ and $j$ are <strong>mutual k-nearest neighbors</strong> if: (1) $j$ is among the $k$ closest points to $i$, AND (2) $i$ is among the $k$ closest points to $j$.
        </p>
      </div>

      <div class="math-block">
        $$\text{CKNNA}_k(K, L) = \text{CKA}(K \odot M_k, L \odot M_k)$$
        <div class="caption">
          $M_k$ is the mutual k-NN mask. Only mutual k-NN pairs contribute to the similarity score.
        </div>
      </div>

      <div class="insight">
        By zeroing out non-local pairs, CKNNA asks: "Do these representations agree on similarity structure <em>within</em> local neighborhoods?"
      </div>
    </section>

    <!-- ==================== RESEARCH DIRECTION ==================== -->
    <section id="research-direction">
      <h2>Our Research Direction</h2>

      <div class="key-question">
        <strong>Key Question 1</strong>
        At what point during training does a model's representation start converging toward its final form?
      </div>

      <div class="key-question">
        <strong>Key Question 2</strong>
        Can we observe the phenomenon of <em>grokking</em> through the lens of representation analysis?
      </div>

      <h3>A Brief Introduction to Grokking</h3>

      <div class="definition">
        <div class="term">Grokking</div>
        <p>
          A phenomenon where neural networks suddenly generalize long after achieving perfect training accuracy. The model appears to "memorize" initially, then undergoes a delayed phase transition to genuine understanding.
        </p>
      </div>

      <p>
        We hypothesize that grokking corresponds to a <strong>representational phase transition</strong>:
      </p>

      <ul>
        <li><strong>Memorization phase:</strong> Complex, data-specific representation.</li>
        <li><strong>Grokking transition:</strong> Representation simplifies and reorganizes.</li>
        <li><strong>Generalization phase:</strong> Converged to something more canonical.</li>
      </ul>

      <div class="insight">
        If this hypothesis is correct, our representational metrics should <em>detect</em> this transition.
      </div>
    </section>

    <!-- ==================== EXPERIMENTS ==================== -->
    <section id="experiments">
      <h2>Experiment Setup</h2>

      <h3>Model: The Pythia Suite</h3>

      <p>
        We use the <strong>Pythia</strong> family of language models, which provides dense checkpoints throughout training.
      </p>

      <ul>
        <li><strong>Multiple scales:</strong> Models from 70M to 12B parameters.</li>
        <li><strong>Consistent training:</strong> All trained on The Pile with same hyperparameters.</li>
        <li><strong>Dense checkpoints:</strong> Steps 0, 1, 2, 4, 8, 16, ..., up to 143,000.</li>
      </ul>

      <h3>Data: WikiText-2</h3>

      <p>
        We use the <strong>WikiText-2-raw-v1</strong> dataset for computing representations:
      </p>

      <ul>
        <li><strong>150 samples</strong> for dynamics experiments (k=16)</li>
        <li><strong>400 samples</strong> for cross-model size comparisons (k=100)</li>
      </ul>

      <h3>Experimental Protocol</h3>

      <h4>Experiment 1: Training Dynamics</h4>
      <p>
        For a single Pythia model, compare each checkpoint's representation against the final checkpoint using both CKA and CKNNA.
      </p>

      <h4>Experiment 2: Cross-Model Scale Comparison</h4>
      <p>
        Compare final representations across Pythia models of different sizes to test PRH predictions.
      </p>

      <h4>Experiment 3: Layer-Wise Analysis</h4>
      <p>
        Repeat dynamics analysis at different layers to understand hierarchical convergence patterns.
      </p>
    </section>

    <!-- ==================== RESULTS ==================== -->
    <section id="results">
      <h2>Experiment Results</h2>

      <h3>Result 1: Training Dynamics Reveal a Surprising Divergence</h3>

      <figure class="figure-wide">
        <img src="images/dynamics-final-layer.png" alt="CKA and CKNNA during training"/>
        <figcaption>
          <strong>Figure 1: Representation Convergence During Training.</strong>
          CKA (blue) shows non-monotonic behavior with a sharp dip around step 20k-40k. CKNNA (orange) remains stable above 0.995 throughout.
        </figcaption>
      </figure>

      <h4>What's Happening Here?</h4>

      <ul>
        <li><strong>Local structure is learned early and preserved.</strong> The stable CKNNA indicates the model quickly learns which points are similar.</li>
        <li><strong>Global structure undergoes reorganization.</strong> The CKA dip shows the overall geometry is changing while local relationships are preserved.</li>
      </ul>

      <div class="insight">
        Think of it like rearranging furniture in a house. Each room (local neighborhood) keeps its layout, but the rooms are being repositioned relative to each other.
      </div>

      <h3>Result 2: Deep Dive into the Reorganization Phase</h3>

      <figure class="figure-wide">
        <img src="images/kernel-analysis-256.png" alt="Kernel analysis at checkpoint 256"/>
        <figcaption>
          <strong>Figure 2: Kernel Analysis at Checkpoint 256.</strong>
          Local pairs correlation (r=0.995) is much tighter than global pairs (r=0.965).
        </figcaption>
      </figure>

      <figure class="figure-wide">
        <img src="images/kernel-analysis-4000.png" alt="Kernel analysis at checkpoint 4000"/>
        <figcaption>
          <strong>Figure 3: Kernel Analysis at Checkpoint 4000.</strong>
          Global pairs have improved to r=0.980 while local pairs remain at r=0.994.
        </figcaption>
      </figure>

      <h3>Result 3: Layer-Specific Convergence Patterns</h3>

      <figure class="figure-wide">
        <img src="images/layer-12-convergence.png" alt="Layer 12 convergence"/>
        <figcaption>
          <strong>Figure 4: Convergence at Layer 12.</strong>
          Middle layers stabilize faster and show less pronounced CKA dips.
        </figcaption>
      </figure>

      <h3>Result 4: Model Scale and Representation Alignment</h3>

      <figure class="figure-wide">
        <img src="images/scale-alignment.png" alt="Alignment vs model size"/>
        <figcaption>
          <strong>Figure 5: Alignment Across Model Sizes.</strong>
          Larger model pairs show higher alignment, supporting the PRH.
        </figcaption>
      </figure>

      <div class="hypothesis-box">
        <div class="label">Key Finding 1: Local Before Global</div>
        <p>Models learn local similarity structure early and preserve it. Global geometry continues to evolve.</p>
      </div>

      <div class="hypothesis-box">
        <div class="label">Key Finding 2: Non-Monotonic Convergence</div>
        <p>There is a distinct reorganization phase where global structure changes while local structure is preserved.</p>
      </div>

      <div class="hypothesis-box">
        <div class="label">Key Finding 3: Scale Drives Convergence</div>
        <p>Larger models show higher mutual alignment, supporting the Platonic Representation Hypothesis.</p>
      </div>
    </section>

    <!-- ==================== REFINED HYPOTHESIS ==================== -->
    <section id="refined-hypothesis">
      <h2>Refining the Platonic Hypothesis</h2>

      <p>
        Our findings suggest a more nuanced formulation: <strong>alignment should happen primarily locally, within semantic neighborhoods</strong>.
      </p>

      <h3>The Lobe Structure of Representations</h3>

      <p>
        Neural networks have multi-scale structure:
      </p>

      <ul>
        <li><strong>Atomic scale:</strong> Local crystal-like structures.</li>
        <li><strong>Intermediate scale:</strong> Semantic "lobes" like brain regions.</li>
        <li><strong>Galaxy scale:</strong> Global organization of lobes.</li>
      </ul>

      <div class="hypothesis-box">
        <div class="label">Refined Platonic Representation Hypothesis</div>
        <p>
          For sufficiently capable models, the representation within each semantic lobe should be equivalent up to <em>local</em> rotation and scaling. Different models may organize globally differently while agreeing locally.
        </p>
      </div>

      <h3>Why This Explains Our Results</h3>

      <ul>
        <li><strong>CKNNA focuses on local neighborhoods</strong>—exactly what should align.</li>
        <li><strong>CKA is dominated by global structure</strong>—where alignment is not guaranteed.</li>
        <li><strong>The CKA dip</strong> corresponds to global reorganization while local structure remains stable.</li>
      </ul>

      <div class="insight">
        The refined hypothesis provides a principled reason to prefer CKNNA: it directly measures what the hypothesis claims should align.
      </div>
    </section>

    <!-- ==================== CONCLUSION ==================== -->
    <section id="conclusion">
      <h2>Conclusion</h2>

      <h3>What We Found</h3>

      <h4>1. Local Before Global</h4>
      <p>Models establish local similarity structure early and preserve it. Global geometry continues evolving.</p>

      <h4>2. Non-Monotonic Convergence</h4>
      <p>There's a distinct "reorganization phase" that may correspond to a transition from memorization to generalization.</p>

      <h4>3. Metrics Reveal Different Stories</h4>
      <p>CKA sees global structure; CKNNA focuses on local neighborhoods. Using both reveals dynamics that either alone would miss.</p>

      <h3>Novel Contributions</h3>

      <ul>
        <li><strong>Dynamics perspective:</strong> Applying representation analysis to learning dynamics, not just fixed models.</li>
        <li><strong>Connection to grokking:</strong> The non-monotonic pattern suggests links between representation reorganization and generalization transitions.</li>
        <li><strong>Refined hypothesis:</strong> Local alignment may be what matters for transfer and generalization.</li>
      </ul>

      <h3>Future Directions</h3>

      <ul>
        <li>Systematic study across model scales.</li>
        <li>Explicit grokking experiments on algorithmic tasks.</li>
        <li>Cross-modal analysis for vision and language models.</li>
        <li>Theoretical foundations for why local stabilizes before global.</li>
      </ul>
    </section>

    <!-- ==================== REFERENCES ==================== -->
    <section id="references">
      <h2>References</h2>

      <ol>
        <li>
          <strong>Huh, M., Cheung, B., Wang, T., & Isola, P. (2024).</strong>
          <em>The Platonic Representation Hypothesis.</em> ICML.
        </li>
        <li>
          <strong>Biderman, S., et al. (2023).</strong>
          <em>Pythia: A Suite for Analyzing Large Language Models.</em> ICML.
        </li>
        <li>
          <strong>Kornblith, S., et al. (2019).</strong>
          <em>Similarity of Neural Network Representations Revisited.</em> ICML.
        </li>
        <li>
          <strong>Gretton, A., et al. (2005).</strong>
          <em>Measuring Statistical Dependence with Hilbert-Schmidt Norms.</em>
        </li>
        <li>
          <strong>Power, A., et al. (2022).</strong>
          <em>Grokking: Generalization Beyond Overfitting.</em> ICLR Workshop.
        </li>
        <li>
          <strong>Anthropic (2023).</strong>
          <em>Scaling Monosemanticity.</em>
        </li>
        <li>
          <strong>Mikolov, T., et al. (2013).</strong>
          <em>Linguistic Regularities in Continuous Space Word Representations.</em>
        </li>
        <li>
          <strong>Merity, S., et al. (2017).</strong>
          <em>Pointer Sentinel Mixture Models.</em> ICLR.
        </li>
      </ol>
    </section>
  </main>

  <footer>
    <div class="footer-content">
      <p>Graduate Course Project · Deep Learning · 2024</p>
    </div>
  </footer>

  <script>
    // Progress bar
    window.addEventListener('scroll', () => {
      const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
      const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
      const scrolled = (winScroll / height) * 100;
      document.getElementById('progress').style.width = scrolled + '%';
    });

    // Active nav link
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('nav a');

    window.addEventListener('scroll', () => {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop;
        if (scrollY >= sectionTop - 100) {
          current = section.getAttribute('id');
        }
      });

      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href').slice(1) === current) {
          link.classList.add('active');
        }
      });
    });
  </script>
</body>
</html>
