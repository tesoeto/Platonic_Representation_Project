<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Evolution of Representation Space: Front And Back</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&family=JetBrains+Mono:wght@400;500&family=Source+Sans+3:wght@400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
        },
      };
    </script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <div class="progress-bar" id="progress"></div>
    <header>
      <div class="header-content">
        <span class="category-tag"
          >Deep Learning • Representation Learning</span
        >
        <h1>Evolution of Representation Space: Front And Back</h1>
        <p class="subtitle">
          Investigating the Platonic Representation Hypothesis through training
          dynamics and representational alignment metrics
        </p>
        <div class="meta">
          <span class="authors">Tony Wu, Baiyu Zhu</span> · 6.7960 Project
        </div>
      </div>
    </header>
    <nav>
      <div class="nav-content">
        <a href="#introduction" class="active">Introduction</a>
        <a href="#formulation">Formulation</a>
        <a href="#metrics">Metrics</a>
        <a href="#experiments">Experiments</a>
        <a href="#results">Results</a>
        <a href="#refined-hypothesis">Refined Hypothesis</a>
        <a href="#conclusion">Conclusion</a>
        <a href="#references">References</a>
      </div>
    </nav>
    <main>
      <div class="toc">
        <h3>Contents</h3>
        <ol>
          <li><a href="#introduction">Introduction</a></li>
          <li>
            <a href="#formulation">Platonic Representation Formulation</a>
          </li>
          <li><a href="#metrics">Similarity Metric Interpretation</a></li>
          <li><a href="#experiments">Experiment Setup</a></li>
          <li><a href="#results">Experiment Results</a></li>
          <li><a href="#refined-hypothesis">A Refined Hypothesis</a></li>
          <li><a href="#conclusion">Conclusion</a></li>
          <li><a href="#references">References</a></li>
        </ol>
      </div>
      <!-- SECTION: Introduction -->
      <section id="introduction">
        <h2>Introduction</h2>

        <p>
          As contrary to using different models to perform different tasks,
          modern Large Language Models (LLM) such as ChatGPT 4 use a single
          unified neural network backbone called the foundation model that's
          often based on Transformer architecture with shared weights to perform
          multiple tasks, such as image caption and text/image generation.
          Instead of having worse performance, when trained on a large corpus of
          data of multimodality, such as both images and texts all at once,
          models generally perform better compared to training on single
          modality. This suggests that there could be a unified representation
          of different modalities that describes the same object.
        </p>

        <p>
          To explain such phenomenon, the
          <strong>Platonic Representation Hypothesis</strong> was introduced
          <span class="citation">[1]</span>. Just as Plato argued
          that our perceived reality consists of imperfect shadows of ideal
          "Forms," the Platonic Representation Hypothesis suggests that
          different models and modalities are approximating the same underlying
          reality from different vantage points. The authors hypothesized that
          there exists an idealized statistical model $P(z)$ that governs the
          events we observe in the universe, and the observations we make about
          objects are only partial information of the original object. Similar
          to the Cave allegory made by Plato, word descriptions and image
          descriptions are merely shadows of the original object. In our blog,
          we study the evolution and alignment of the representation space and
          use the Platonic Representation Hypothesis to explain our findings,
          and in the appendix we present a refined platonic representation
          hypothesis, taking into account of the recent work that shows neurons
          which fire to similar concepts tend to form local clusters that
          resemble lobes of the brain
          <span class="citation">[9]</span>.
        </p>

        <div class="key-question">
          <strong>Central Investigation</strong>
          We run experiments to demonstrate the evolution of the alignment of
          the model over training epoch. We observe that the first few layers
          align quickly, while for the later layers we observe a phase of
          alignment dip and plateau followed by a sharp transition of increase
          in alignment.
        </div>

        <h3>The Linear Representation Hypothesis</h3>

        <p>
          It's widely accepted in the ML community that direction of the
          representation space embeds semantic meaning, and that we have an
          approximate linear relation of $\text{King} - \text{Queen} \approx
          \text{Man} - \text{Woman}$<span class="citation">[10]</span>. Mathematically, it's regarded
          that each feature is represented by a unit vector $d_i$ in the
          representation space, and the general vector can be written as $\sum_i
          a_i d_i$. This is known as the
          <strong>linear representation hypothesis</strong>. However, this
          relation only holds approximately.
        </p>

        <h3>Scales of Representation Space</h3>

        <p>
          To further understand the representation space, tools such as Sparse
          Auto Encoder (SAE) were introduced, where we take the first k layers
          of the LLM and introduce an Auto-Encoder with the loss designed to
          make the features sparse. We denote $x$ to be the input to SAE, $f(x)
          = \text{Encoder}(x)$ to be the latent representation, and $\hat{x} =
          \text{Decoder}(f)$ to be the reconstructed vector. We freeze the first
          k layers and train the head by the following loss:
        </p>

        <div class="math-block">
          $$\mathcal{L} = \mathbb{E}_{x \sim \mathcal{D}}\left[ ||x -
          \hat{x}||_2^2 + \lambda ||f||_1 \right]$$
          <div class="caption">
            The first term is the reconstruction loss ensuring the autoencoder
            preserves information. The second term enforces sparsity,
            encouraging the model to use only a few features at once.
          </div>
        </div>

        <p>
          Using SAE, it's been explored that there are
          <strong>three scales</strong> of the representation space: Atomic,
          Intermediate, and Galaxy scales<span class="citation">[9]</span>. The atomic scale contains
          crystal structures with parallelogram or trapezoid faces that
          approximately satisfy the linear representation hypothesis. In the
          intermediate scale, it's observed that neurons that fire to similar
          concepts, such as coding and math, are clustered together—similar to
          lobes of the brain seen in fMRI images.
        </p>

        <div class="insight">
          Nearby vectors in representation space are observed to have similar
          semantic meanings. This suggests a sense of <em>smoothness</em> of the
          embedding of datapoints for large enough models trained over many
          epochs. Given a point $x$ in the representation space and sufficiently
          small radius $\epsilon > 0$, all the points in the ball $B_\epsilon(x)
          \equiv \{y \mid |x - y| < \epsilon\}$ have similar semantic meanings.
        </div>
      </section>

      <!-- SECTION: Platonic Representation Formulation -->
      <section id="formulation">
        <h2>Platonic Representation Formulation</h2>

        <p>
          We provide an overview of previous established concepts in
          literatures. A vector representation of an input $x \in \mathcal{X}$
          is defined to be a mapping from the data space $\mathcal{X}$ to the
          target space $\mathbb{R}^n$, provided by the formula:
        </p>

        <div class="math-block">
          $$f: \mathcal{X} \rightarrow \mathbb{R}^n$$
          <div class="caption">
            The representation function maps each input from the data space to a
            point in n-dimensional Euclidean space. This is the fundamental
            object we study when comparing models.
          </div>
        </div>

        <p>
          To characterize the representation space, kernel $\mathcal{K}$ is
          introduced, which measures the similarity between the representations
          of the data points as:
        </p>

        <div class="math-block">
          $$\mathcal{K}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R},
          \quad \mathcal{K}(x_i, x_j) = \langle f(x_i), f(x_j) \rangle$$
          <div class="caption">
            The kernel $K(x_i, x_j)$ calculates the similarities between the
            different representations of the datapoints using inner product, allowing
            us to analyse the geometry of the representation space more generally than just comparing high dimensional vectors. 
          </div>
        </div>

        <p>
          Then, given two models with different representations $f$ and $g$, we
          can calculate the similarity between the representations by designing
          a similarity metric between the two kernels $m: \mathcal{K} \times
          \mathcal{K} \rightarrow \mathbb{R}$. Common similarity metrics include
          the CKA and nearest neighbor metric, while in the original platonic
          representation work, the authors used CKNNA.
        </p>

        <h3>Empirical Observations on Scale</h3>

        <p>
          Previous work showed that given sufficient amount of data, as we scale
          up two different models A and B, the representations are more aligned
          for the larger models compared to the smaller models. The authors
          provided the following hypotheses to explain for such observation:
        </p>

        <div class="hypothesis-box">
          <div class="label">Hypothesis 1: Convergence via Task Generality</div>
          <p>
            As we increase the number of data points, the number of constraints
            put on the model increases if it performs well across the dataset.
            This restricts the possible hypothesis space to a small region.
          </p>
        </div>

        <div class="hypothesis-box">
          <div class="label">Hypothesis 2: Convergence via Model Capacity</div>
          <p>
            As we increase the model capacity, the representation of the model
            increases so it has the capacity to find a more optimal solution.
            Larger models can explore a richer function space and are more
            likely to discover the "true" underlying structure.
          </p>
        </div>

        <div class="hypothesis-box">
          <div class="label">Hypothesis 3: The Simplicity Bias</div>
          <p>Larger models tend to find simpler solutions to the problems.</p>
        </div>

        <div class="insight">
          It's plausible that the third hypothesis is related to
          Grokking<span class="citation">[4]</span>, where the model first remembers the datasets then
          as the training epoch increases, the model tends to a simpler
          solution. This suggests some form of implicit regularization driving
          models toward more compressed representations.
        </div>
      </section>

      <!-- SECTION: Similarity Metric Interpretation -->
      <section id="metrics">
        <h2>Similarity Metric Interpretation</h2>

        <p>
          Naively, if there exists a bijective map between two representations X
          and Y, they should encode the same amount of information so it's
          expected that they should perform the same, given an optimal decoder<span class="citation">[11]</span>.
          As it is the case that invertible linear transformations belong to
          this set, we should expect that a good kernel should be invariant
          under any transformation $T \in \text{GL}_n(\mathbb{R})$. However, as
          argued in the literature<span class="citation">[3]</span>, this would imply that scale of directions
          doesn't matter.
        </p>

        <p>
          Instead of requiring invariance under elements of
          $\text{GL}_n(\mathbb{R})$, only orthogonal transformation and scale
          invariance are required. One choice is the CKA metric:<span class="citation">[3]</span>
        </p>

        <div class="math-block">
          $$\text{CKA}(K, L) = \frac{\text{HSIC}(K, L)}{\sqrt{\text{HSIC}(K, K)
          \cdot \text{HSIC}(L, L)}}$$ $$\text{HSIC}(K, L) = \frac{1}{(n-1)^2}
          \text{tr}(KHLH)$$
          <div class="caption">
            Where $H_n = I_n - \frac{1}{n}\mathbf{1}\mathbf{1}^T$ is the
            centering matrix, while K and L are different kernels. CKA
            normalizes HSIC to produce a similarity score between 0 and 1.
          </div>
        </div>

        <h3>Understanding HSIC: The Core Building Block</h3>

        <p>
          The Hilbert-Schmidt Independence Criterion (HSIC) measures statistical
          dependence between two kernel matrices. The intuition is that if two
          representations capture similar structure, then knowing the similarity
          between two points in one representation should tell us about their
          similarity in the other representation.
        </p>

        <h3>Why CKA Falls Short: The Global Domination Problem</h3>

        <p>
          However, results have shown that CKNNA is a better metric at capturing
          the alignment between the kernels.
          <strong
            >Intuitively, two vector representations that are far from each
            other should not affect each other too greatly.</strong
          >
          This is the key insight that motivates using local metrics.
        </p>

        <p>
          CKA computes similarity over <em>all</em> pairs of points, which means
          it's sensitive to global structure. Consider two representations that
          organize data into the same global clusters but differ in fine-grained
          local structure. CKA will report higher similarity because the
          large-scale structure influences the sum. But local structure often
          matters more for downstream tasks: knowing which specific points are
          most similar to a query is crucial for retrieval, few-shot learning,
          and more.
        </p>

        <h3>CKNNA: Focusing on Local Structure</h3>

        <p>
          Centered Kernel Nearest Neighbor Alignment (CKNNA) addresses CKA's
          limitations by restricting the comparison to local neighborhoods. The
          key idea is to only consider pairs of points that are mutual k-nearest
          neighbors.
        </p>

        <div class="insight">
          By comparing CKA and CKNNA, we can distinguish between changes in
          global structure (CKA drops, CKNNA stable) and changes in local
          structure (both drop). This distinction proves crucial for
          understanding training dynamics.
        </div>
      </section>

      <!-- SECTION: Experiment Setup -->
      <section id="experiments">
        <h2>Experiment Setup</h2>

        <h3>Model: Pythia</h3>

        <p>
          We use the <strong>Pythia</strong> suite of language models
          <span class="citation">[2]</span>, which provides
          a unique resource for studying training dynamics. Unlike most released
          models, Pythia checkpoints saved throughout training,
          allowing us to analyze how representations evolve over time.
        </p>

        <p>Key properties of Pythia that make it ideal for our study:</p>

        <ul>
          <li>
            <strong>Multiple scales:</strong> Models range from 70M to 12B
            parameters, enabling us to test how scale affects representation
            convergence.
          </li>
          <li>
            <strong>Consistent training:</strong> All models trained on the same
            data (The Pile) with the same hyperparameters (relative to scale),
            isolating the effect of model size.
          </li>
          <li>
            <strong>Dense checkpoints:</strong> Checkpoints available at steps
            0, 1, 2, 4, 8, 16, ..., 143,000, providing fine-grained visibility
            into training dynamics.
          </li>
        </ul>

        <h3>Dataset: WikiText-2</h3>

        <p>
          For our experiments, we use text samples from the
          <strong>WikiText-2-raw-v1</strong> dataset. This dataset provides
          high-quality Wikipedia articles that serve as a good test bed for
          analyzing language model representations. The text is relatively clean
          and covers diverse topics, making it suitable for probing general
          language understanding.
        </p>

        <h3>Experimental Protocol</h3>

        <p>
          Our experiments follow a consistent protocol designed to isolate the
          effect of training progress on representation alignment:
        </p>

        <ol>
          <li>
            <strong>Select checkpoints:</strong> We compare representations at
            checkpoint $t$ against the final checkpoint to measure convergence
            toward the "mature" representation.
          </li>
          <li>
            <strong>Extract representations:</strong> For a fixed set of input
            examples, we extract the hidden states at specific layers. We obtain
            representations by mean-pooling across sequence positions.
          </li>
          <li>
            <strong>Compute kernel matrices:</strong> Using linear kernel,
            compute pairwise similarity matrices $K_t$ and $K_{\text{final}}$.
          </li>
          <li>
            <strong>Compute alignment:</strong> Calculate both CKA and CKNNA
            between $K_t$ and $K_{\text{final}}$.
          </li>
        </ol>

        <h3>Experiment Configurations</h3>

        <p>We run two main types of experiments:</p>

        <div class="hypothesis-box">
          <div class="label">Experiment 1: Training Dynamics</div>
          <p>
            We track how representation alignment evolves across training
            checkpoints for a single model. Using
            <strong>150 samples</strong> from WikiText-2 with
            <strong>k=16</strong> for CKNNA, we measure alignment at checkpoints
            from step 4,000 to step 128,000 against the final trained model.
          </p>
        </div>

        <div class="hypothesis-box">
          <div class="label">Experiment 2: Model Scale</div>
          <p>
            We compare representations across Pythia models of different sizes
            (70M, 160M, 410M, 1B, 1.4B, 2.8B) at their final checkpoints. Using
            <strong>400 samples</strong> with <strong>k=100</strong> for CKNNA,
            we measure pairwise alignment to test whether larger models converge
            to more similar representations.
          </p>
        </div>

        <h3>Why These Choices?</h3>

        <p>
          The parameter choices balance statistical stability with computational
          feasibility:
        </p>

        <ul>
          <li>
            <strong>k=16 for dynamics:</strong> With 150 samples, k=16 means
            each point's neighborhood contains about 10% of the data—large
            enough to be statistically meaningful but small enough to reflect
            genuinely local structure.
          </li>
          <li>
            <strong>k=100 for scale:</strong> With 400 samples, k=100 provides
            richer neighborhood information while still focusing on local
            relationships.
          </li>
          <li>
            <strong>Final checkpoint as reference:</strong> We measure alignment
            relative to the final trained model, treating it as the "converged"
            representation against which we measure progress.
          </li>
        </ul>

        <div class="insight">
          Computing the mutual k-NN mask from the final checkpoint ensures we're
          asking a consistent question: "How well does this intermediate
          representation capture the neighborhood structure that the model
          eventually learns?"
        </div>
      </section>

      <!-- SECTION: Experiment Results -->
      <section id="results">
        <h2>Experiment Results</h2>

        <h3>Training Dynamics: The Surprising Non-Monotonicity of CKA</h3>

        <p>
          Our first experiment tracks how representation alignment evolves
          throughout training. We compare each checkpoint's representation
          against the final trained model, asking: "How close is this
          intermediate representation to what the model eventually learns?"
        </p>

        <figure class="figure-wide">
          <img
            src="assets/images/dynamics-final-layer.png"
            alt="Representation convergence analysis showing CKA and CKNNA metrics over training steps"
          />
          <figcaption>
            <strong>Figure 1: Training Dynamics at the Last Layer.</strong>
            Both CKA (blue) and CKNNA (orange, k=16) are plotted against
            training step count. The striking finding: CKNNA starts high
            (~0.995) and remains remarkably stable throughout training, while
            CKA shows dramatic non-monotonic behavior—rising initially, dropping
            sharply around step 20,000-40,000, then recovering. This
            dissociation reveals that <em>global</em> structure undergoes
            significant reorganization while <em>local</em> neighborhood
            structure remains preserved.
          </figcaption>
        </figure>

        <h4>What Does This Divergence Mean?</h4>

        <p>
          The dramatic difference between CKA and CKNNA reveals something
          profound about how neural networks learn. Around step 20,000-40,000,
          the model undergoes what we call a
          <strong>global reorganization phase</strong>:
        </p>

        <ul>
          <li>
            <strong>Local structure is preserved:</strong> CKNNA remains above
            0.995 throughout training. The model consistently identifies which
            data points are similar to each other—the "who is near whom"
            question has a stable answer even as the overall geometry shifts.
          </li>
          <li>
            <strong>Global structure is reorganized:</strong> CKA drops to
            ~0.982, indicating the overall shape of the representation manifold
            is being restructured. Distant points are changing their
            relationships even as nearby points maintain theirs.
          </li>
          <li>
            <strong>Eventual convergence:</strong> Both metrics converge toward
            1.0 by the end of training, confirming that the final representation
            achieves both local and global stability.
          </li>
        </ul>

        <div class="insight">
          ********* TO BE ADDED *********
        </div>

        <h3>Deep Dive: Kernel Matrix Analysis</h3>

        <p>
          To understand what drives the CKA drop, we examine the kernel matrices
          at two key checkpoints: early training (step 256) and mid-training
          (step 4000).
        </p>

        <p> Need to add experiment around step 20000-50000 of reorganisation period</p>

        <figure class="figure-wide">
          <img
            src="assets/images/kernel-analysis-256.png"
            alt="Detailed kernel analysis at checkpoint 256"
          />
          <figcaption>
            <strong
              >Figure 2: Kernel Analysis at Checkpoint 256 (Early
              Training).</strong
            >
            <strong>Top row:</strong> Kernel matrices K (checkpoint 256), L
            (final checkpoint), and mutual kNN mask (k=16). The checkered
            pattern in K and L reveals cluster structure in the data.
            <strong>Bottom row:</strong> Scatter plots showing local pairs
            correlation (r=0.995) versus global pairs correlation (r=0.965),
            plus the absolute difference |K-L|. The 3% gap between local and
            global correlation is the signature of global reorganization—nearby
            points agree strongly, but distant points show more divergence.
          </figcaption>
        </figure>

        <p>
          The kernel matrices reveal a clear structure: both K and L show
          similar "checkered" patterns indicating cluster organization in the
          data. The bright spots along the diagonal and at certain off-diagonal
          positions correspond to groups of semantically similar text passages.
          What's remarkable is how similar these structures are even at step
          256—the model has already learned which points belong together.
        </p>

        <figure class="figure-wide">
          <img
            src="assets/images/kernel-analysis-4000.png"
            alt="Detailed kernel analysis at checkpoint 4000"
          />
          <figcaption>
            <strong
              >Figure 3: Kernel Analysis at Checkpoint 4000
              (Mid-Training).</strong
            >
            Same analysis as Figure 2 but at step 4000. The global pairs
            correlation has improved to 0.980 (up from 0.965), while local pairs
            remain at 0.994. The |K-L| difference map shows similar structure to
            checkpoint 256, but with generally smaller magnitudes. The model is
            converging toward the final representation, with global structure
            catching up to local structure.
          </figcaption>
        </figure>

        <h4>The Local-Global Gap</h4>

        <p>
          The scatter plots in Figures 2 and 3 reveal the key insight:
          <strong
            >local pairs (mutual k-NN) show tighter correlation than global
            pairs (all pairs)</strong
          >. At checkpoint 256:
        </p>

        <ul>
          <li>Local pairs correlation: <strong>r = 0.995</strong></li>
          <li>Global pairs correlation: <strong>r = 0.965</strong></li>
        </ul>

        <p>
          This 3% gap explains exactly why CKNNA stays high while CKA dips.
          CKNNA only "sees" the local pairs where agreement is near-perfect. CKA
          sees all pairs, including the global ones that are still being
          reorganized.
        </p>

        <h3>Layer-Specific Analysis: Middle vs. Final Layers</h3>

        <p>
          The Platonic Representation Hypothesis suggests that later layers
          should show different convergence dynamics, as they contain more
          abstract, task-relevant representations. We test this by analyzing
          convergence at layer 12 (middle of the network).
        </p>

        <figure class="figure-wide">
          <img
            src="assets/images/layer-12-convergence.png"
            alt="Representation convergence at layer 12"
          />
          <figcaption>
            <strong
              >Figure 4: Convergence Analysis at Layer 12 (Middle
              Layer).</strong
            >
            The same CKA/CKNNA analysis at a middle layer shows qualitatively
            different dynamics. Most notably:
            <strong>no reorganization dip</strong>. Both metrics start high and
            converge smoothly toward 1.0. The representation restructuring we
            observed in the final layer doesn't appear here.
          </figcaption>
        </figure>

        <h4>Why Do Layers Behave Differently?</h4>

        <p>
          The contrast between layer 12 and the final layer tells us something
          important about how information flows through the network:
        </p>

        <ul>
          <li>
            <strong>Middle layers stabilize earlier:</strong> Layer 12 shows
            smooth convergence without the dramatic reorganization phase. The
            representations here reach their final form more directly.
          </li>
          <li>
            <strong>Final layers are more dynamic:</strong> The last layer
            undergoes more substantial restructuring during training. This makes
            sense—later layers are responsible for task-specific processing and
            must adapt more dramatically to capture the full complexity of the
            training objective.
          </li>
          <li>
            <strong>Early convergence of local structure:</strong> In both
            layers, CKNNA is consistently above CKA, confirming that local
            structure converges before global structure regardless of layer
            depth.
          </li>
        </ul>

        <div class="insight">
          This layer-wise analysis suggests a <em>hierarchical</em> convergence
          process: earlier layers establish stable, reusable features early in
          training, while later layers continue to reorganize how these features
          are combined for the final task.
        </div>

        <h3>Model Scale: Larger Models, More Aligned Representations</h3>

        <p>
          Our second experiment tests the core prediction of the Platonic
          Representation Hypothesis: that larger models should converge to more
          similar representations.
        </p>

        <figure class="figure-wide">
          <img
            src="assets/images/scale-alignment.png"
            alt="Representation alignment for models of different sizes"
          />
          <figcaption>
            <strong
              >Figure 5: Representation Alignment Across Model Scales.</strong
            >
            We compare final-checkpoint representations across Pythia models of
            different sizes, plotting representation alignment (y-axis) against
            log model size (x-axis). Using 400 samples with k=100 for CKNNA.
            Both CKA and CKNNA show a clear trend:
            <strong>larger models produce more aligned representations</strong>.
            CKNNA (k=16 and k=100) consistently exceeds CKA, confirming that
            local structure is more aligned than global structure even across
            model scales.
          </figcaption>
        </figure>

        <h4>Key Observations on Scale</h4>

        <ul>
          <li>
            <strong>Monotonic improvement with scale:</strong> Both CKA and
            CKNNA increase as model size grows. The 2.8B parameter model shows
            near-perfect alignment (>0.999) while the 70M model shows lower
            alignment (~0.983 for CKA).
          </li>
          <li>
            <strong>CKNNA consistently higher:</strong> Across all model sizes,
            CKNNA exceeds CKA by 1-2%. This confirms that local neighborhoods
            are better aligned than global structure, even when comparing
            fully-trained models.
          </li>
          <li>
            <strong>Diminishing returns at scale:</strong> The gap between
            consecutive model sizes shrinks as we move to larger models. The
            jump from 70M to 160M is substantial; the jump from 1B to 1.4B is
            smaller.
          </li>
        </ul>

        <div class="insight">
          The scale results support the Platonic Representation Hypothesis: as
          models become more capable, they converge toward similar
          representations. The local-before-global pattern we observed in
          training dynamics also appears in the scale comparison—local structure
          is more universal than global structure.
        </div>

        
        </ol>

      </section>

      <!-- SECTION: Refined Hypothesis -->
      <section id="refined-hypothesis">
        <h2>A Refined Hypothesis</h2>

        <p>
          We refine the platonic hypothesis to account for the recent
          discoveries made by Anthropic and others, while also trying to
          reconcile with the finding that a well-performing neural network may
          resemble that of a human brain. We also try to incorporate the
          hypothesis to explain why CKNNA performs better.
        </p>

        <h3>The Lobe Structure of Representation Space</h3>

        <p>
          We develop some intuitions by considering a toy model: Let's consider
          grouping of locally similar neighbors $L_i$ that have similar
          representations together and represent each neighbor with a node. In
          other words, we group feature vectors that have similar meaning as a
          single point. We could group the representation vectors in the same
          spirit as K-means or the EM algorithm.
        </p>

        <p>
          But to make sure that we can compare two different models, we need to
          pick the initial points carefully. One way we can do this is: for each
          task $x_i$, we identify which neuron fires the most and pick such
          vector as the starting point, and we do this for each datapoint in
          $\{x_i\}$. We also connect the different nodes using edges $e \in E$,
          with the interpretation that the edges are used to interchange
          information with each other when solving tasks.
        </p>

        <div class="hypothesis-box">
          <div class="label">Central Hypothesis: Local Alignment</div>
          <p>
            Instead of aligning globally, the alignment should happen primarily
            <strong>locally</strong>. We could treat each neighborhood as a
            subspace and demand invariance under rotation and local scaling.
          </p>
        </div>

        <h3>Mathematical Formulation</h3>

        <p>
          We identify the mean of the lobe $\mu_x$ and demand that the
          representation be invariant under rotation and scaling
          <em>relative to the mean within the cluster</em>. Specifically, the
          vector space $\{x - \mu_i \mid x \in L_i\}$ should be equivalent to
          $\{\alpha T(x - \mu_i) \mid x \in L_i\}$ with $T$ being a rotation and
          $\alpha$ being a scaling factor.
        </p>

        <p>
          To preserve the linear representation hypothesis roughly, we modify
          the classic analogy equation as:
        </p>

        <div class="math-block">
          $$L(\text{King}) - L(\text{Queen}) \approx L(\text{Man}) -
          L(\text{Woman})$$
          <div class="caption">
            Where $L(\cdot)$ denotes the cluster centroid of the input. This
            makes sense as long as the lobe is small enough to align with the
            linear representation hypothesis locally.
          </div>
        </div>

        <h3>Why This Explains CKNNA's Superiority</h3>

        <p>
          Our hypothesis helps to explain why CKNNA performs better than CKA:
          <strong
            >CKNNA extracts only the k-nearest neighbor points that are likely
            to belong to the same neighborhood, while ignoring vectors that are
            far apart from each other that share less similarities.</strong
          >
        </p>

        <p>
          The key insight is that different models may organize their global
          geometry differently—one model might have elongated clusters, another
          might have spherical ones—but within each cluster, the relative
          positions of points are more constrained by the task. CKA sees the
          global differences and reports lower alignment. CKNNA sees only the
          local agreement and reports higher alignment.
        </p>

        <div class="insight">
          This also connects to neuroscience: the brain organizes knowledge into
          localized "lobes" that specialize in different functions. If neural
          networks are converging toward some universal representation, it makes
          sense that this convergence would manifest locally—at the level of
          specialized "neighborhoods"—before appearing globally.
        </div>

        <h3>Implications for the Platonic Representation Hypothesis</h3>

        <p>
          Our refined hypothesis suggests a more nuanced version of the PRH:
        </p>

        <ul>
          <li>
            <strong>Local convergence is primary:</strong> The "Platonic ideal"
            that models converge toward may be better understood as a
            <em>local</em> property—agreement on similarity structure within
            neighborhoods—rather than a global geometric constraint.
          </li>
          <li>
            <strong>Global structure is under-determined:</strong> The data may
            not uniquely specify how neighborhoods should be arranged relative
            to each other. This explains why different models can achieve
            similar performance with different global geometries.
          </li>
          <li>
            <strong>Scale enables global alignment:</strong> Larger models may
            achieve better global alignment simply because they have the
            capacity to discover a more "canonical" arrangement of the
            locally-aligned neighborhoods.
          </li>
        </ul>
      </section>

      <!-- SECTION: Conclusion -->
      <section id="conclusion">
        <h2>Conclusion</h2>

        <h3>Key Findings</h3>

        <p>
          Our experiments reveal several important insights about
          representational dynamics during neural network training:
        </p>

        <ol>
          <li>
            <strong>Local before global:</strong> Models establish local
            similarity structure early in training, while global geometry
            continues to evolve. CKNNA consistently exceeds CKA throughout
            training, confirming that local neighborhoods align before—and more
            completely than—global structure.
          </li>
          <li>
            <strong>Non-monotonic convergence:</strong> Representation alignment
            is not monotonic. At the final layer, we observe a distinct
            reorganization phase (around steps 20k-50k) where global structure
            changes significantly while local structure is preserved. This
            suggests the model undergoes a "restructuring" phase during
            training.
          </li>
          <li>
            <strong>Layer-specific dynamics:</strong> Middle layers (layer 12)
            converge smoothly without the dramatic reorganization seen in final
            layers. This suggests a hierarchical learning process where earlier
            layers stabilize first.
          </li>
          <li>
            <strong>Scale enhances alignment:</strong> Larger models produce
            more aligned representations, supporting the core claim of the
            Platonic Representation Hypothesis. The local-before-global pattern
            persists across scales.
          </li>
        </ol>

        <h3>Novel Contributions</h3>

        <ul>
          <li>
            <strong>Training dynamics perspective:</strong> We apply
            representation analysis to study <em>learning dynamics</em>, not
            just compare trained models. This reveals structure in how
            representations evolve that static comparisons miss.
          </li>

          <li>
            <strong>Refined local alignment hypothesis:</strong> Our theoretical
            framework suggests that convergent representations should be
            understood primarily as <em>local</em> phenomena, with global
            alignment emerging as a secondary effect of scale and capacity.
          </li>
        </ul>

        <h3>Future Directions</h3>

        <ul>
          <li>
            <strong>Systematic scale study:</strong> Compare representation
            convergence across all Pythia sizes with finer checkpoint
            granularity to quantify how model capacity affects both the rate and
            final degree of alignment.
          </li>
          <li>
            <strong>Cross-modal analysis:</strong> Test whether vision and
            language models show similar local-before-global convergence
            patterns, and whether their representations become more aligned at
            larger scales.
          </li>
          <li>
            <strong>Theoretical analysis:</strong> Develop a formal
            understanding of why local structure is preserved during global
            reorganization—what loss landscape properties or optimization
            dynamics produce this pattern?
          </li>
        </ul>
      </section>

      <!-- SECTION: References -->
      <section id="references">
        <h2>References</h2>

        <ol class="references-list">
          <li>
            <span class="ref-authors"
              >Huh, M., Cheung, B., Wang, T., & Isola, P.</span
            >
            (2024).
            <span class="ref-title"
              >The Platonic Representation Hypothesis.</span
            >
            <span class="ref-venue">ICML 2024.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Biderman, S., Schoelkopf, H., Anthony, Q., et al.</span
            >
            (2023).
            <span class="ref-title"
              >Pythia: A Suite for Analyzing Large Language Models Across
              Training and Scaling.</span
            >
            <span class="ref-venue">ICML 2023.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Kornblith, S., Norouzi, M., Lee, H., & Hinton, G.</span
            >
            (2019).
            <span class="ref-title"
              >Similarity of Neural Network Representations Revisited.</span
            >
            <span class="ref-venue">ICML 2019.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra,
              V.</span
            >
            (2022).
            <span class="ref-title"
              >Grokking: Generalization Beyond Overfitting on Small Algorithmic
              Datasets.</span
            >
            <span class="ref-venue">ICLR 2022.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Gretton, A., Bousquet, O., Smola, A., & Schölkopf, B.</span
            >
            (2005).
            <span class="ref-title"
              >Measuring Statistical Dependence with Hilbert-Schmidt
              Norms.</span
            >
            <span class="ref-venue">Algorithmic Learning Theory.</span>
          </li>
          <li>
            <span class="ref-authors">Anthropic.</span> (2024).
            <span class="ref-title"
              >Scaling Monosemanticity: Extracting Interpretable Features from
              Claude 3 Sonnet.</span
            >
            <span class="ref-venue">Anthropic Research.</span>
          </li>
          <li>
            <span class="ref-authors">Templeton, A., et al.</span> (2024).
            <span class="ref-title"
              >Scaling and Evaluating Sparse Autoencoders.</span
            >
            <span class="ref-venue">Anthropic Research.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean,
              J.</span
            >
            (2013).
            <span class="ref-title"
              >Distributed Representations of Words and Phrases and their
              Compositionality.</span
            >
            <span class="ref-venue">NeurIPS 2013.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Li, Y., Michaud, E. J., Baek, D. D., Engels, J., Sun, X., & Tegmark,
              M.</span
            >
            (2025).
            <span class="ref-title"
              >The Geometry of Concepts: Sparse Autoencoder Feature Structure.</span
            >
            <span class="ref-venue">Entropy 27(4).</span>
          </li>
          <li>
            <span class="ref-authors">Mikolov, T., Chen, K., Corrado, G., & Dean, J.</span>
            (2013).
            <span class="ref-title">Efficient Estimation of Word Representations in Vector Space.</span>
            <span class="ref-venue">ICLR 2013.</span>
          </li>
          <li>
            <span class="ref-authors"
              >Li, Y., Yosinski, J., Clune, J., Lipson, H., and Hopcroft, J.</span
            >
            (2015).
            <span class="ref-title"
              >Convergent learning: Do different neural networks learn the same
              representations?</span
            >
            <span class="ref-venue"
              >NIPS 2015 Workshop on Feature Extraction: Modern Questions and
              Challenges.</span
            >
          </li>
        </ol>
      </section>
    </main>
    <footer>
      <div class="footer-content">
        <p>6.7960 Project</p>
      </div>
    </footer>
    <script>
      window.addEventListener("scroll", () => {
        const winScroll =
          document.body.scrollTop || document.documentElement.scrollTop;
        const height =
          document.documentElement.scrollHeight -
          document.documentElement.clientHeight;
        const scrolled = (winScroll / height) * 100;
        document.getElementById("progress").style.width = scrolled + "%";
      });
      const sections = document.querySelectorAll("section[id]");
      const navLinks = document.querySelectorAll("nav a");
      window.addEventListener("scroll", () => {
        let current = "";
        sections.forEach((section) => {
          const sectionTop = section.offsetTop;
          if (scrollY >= sectionTop - 100) {
            current = section.getAttribute("id");
          }
        });
        navLinks.forEach((link) => {
          link.classList.remove("active");
          if (link.getAttribute("href").slice(1) === current) {
            link.classList.add("active");
          }
        });
      });
    </script>
  </body>
</html>
